"""
Run Nagios CPU usage machines
"""
import os
from datetime import date, datetime
import random
import json

from . import settings
from .data_source.db_query import get_cpu_query
from .data_source.dataframe import db_to_df, hosts_timeseries, normalized_cpu_threshold_df
from .clustering.clustering import run_clustering
from .outputs.outputs import write_clustering_results

def _json_default(obj):
    """
    Convert datetimes to string for json export
    """
    if isinstance(obj, (date, datetime)):
        return obj.isoformat()

def dataset_and_clustering():

    """
    Create a filtered and formated dataset according to settings and run clustering
    """
    start = datetime.now()

    dataset_start = datetime.now()
    print('{} Fetching dataset'.format(dataset_start))
    # DB query
    query = get_cpu_query(
        **settings.DB_QUERY_ARGS
    )
    records = query.all()

    # Convert DB records to dataframe
    initial_df = db_to_df(records)

    # Select a number of random machines if specified
    if settings.NUMBER_OF_RANDOM_MACHINES:
        machines = random.sample(
            list(initial_df.hostname.unique()),
            k=settings.NUMBER_OF_RANDOM_MACHINES
        )
        initial_df = initial_df[initial_df.hostname.isin(machines)]

    # Normalize CPU values against specified thresholds if specified
    if settings.CPU_THRESHOLD_NORMALIZATION:
        normalized_cpu_df = normalized_cpu_threshold_df(
            initial_df,
            settings.CPU_THRESHOLD_NORMALIZATION
        )
    else:
        normalized_cpu_df = initial_df

    # Get tslearn compatible timeseries, with missing dates
    # filled automatically
    # Increase time series interval if specified
    timeseries = hosts_timeseries(
        normalized_cpu_df,
        interval_seconds=settings.INTERVAL_SECONDS
    )
    dataset_end = datetime.now()
    print('{} Dataset fetched'.format(dataset_end))
    dataset_duration = int((dataset_end - dataset_start).total_seconds())
    print('Dataset duration: {}s'.format(dataset_duration))
    print()

    clustering_start = datetime.now()
    print('{} Starting clustering'.format(clustering_start))
    tskm = run_clustering(timeseries, **settings.TSKM_ARGS)
    clustering_end = datetime.now()
    print('{} Finished clustering'.format(clustering_end))
    clustering_duration = int((clustering_end - clustering_start).total_seconds())
    print('Clustering duration: {}s'.format(clustering_duration))
    print()

    # Convert output directory to absolute path
    results_directory = os.path.abspath(settings.RESULTS_DIRECTORY)

    # Create output directory if not existing
    if not os.path.isdir(results_directory):
        os.makedirs(results_directory)

    results_start = datetime.now()
    print('{} Writing results to {}'.format(results_start, results_directory))
    write_clustering_results(
        initial_df,
        normalized_cpu_df,
        tskm,
        results_directory
    )
    results_end = datetime.now()
    print('{} Results written'.format(results_end))
    results_duration = int((results_end - results_start).total_seconds())
    print('Results duration: {}s'.format(results_duration))
    print()

    end = datetime.now()
    duration = int((end - start).total_seconds())
    print('Finished')
    print('Total duration: {}s'.format(duration))

    # Write summary to output directory
    summary = {
        'start_date': start,
        'settings': {
            'DB_QUERY_ARGS': settings.DB_QUERY_ARGS,
            'NUMBER_OF_RANDOM_MACHINES': settings.NUMBER_OF_RANDOM_MACHINES,
            'CPU_THRESHOLD_NORMALIZATION': settings.CPU_THRESHOLD_NORMALIZATION,
            'INTERVAL_SECONDS': settings.INTERVAL_SECONDS,
            'TSKM_ARGS': settings.TSKM_ARGS,
            'RESULTS_DIRECTORY': results_directory,
        },
        'durations': {
            'dataset': dataset_duration,
            'clustering': clustering_duration,
            'results': results_duration,
            'total': duration,
        }
    }
    with open(os.path.join(results_directory, 'summary.json'), 'w') as summary_file:
        json.dump(summary, summary_file, sort_keys=False, indent=4, default=_json_default)


def only_clustering(dic_settings):

    """
    Create a filtered and formated dataset according to settings and run clustering
    """

    settings.DB_QUERY_ARGS = dic_settings['db_query_args']
    settings.NUMBER_OF_RANDOM_MACHINES = dic_settings['number_of_random_machines']
    settings.CPU_THRESHOLD_NORMALIZATION = dic_settings['cpu_threshold_normalization']
    settings.INTERVAL_SECONDS = dic_settings['interval_seconds']
    settings.TSKM_ARGS = dic_settings['tskm_args']
  

    dataset_start = datetime.now()
    print('{} Fetching dataset'.format(dataset_start))
    # DB query
    query = get_cpu_query(
        **settings.DB_QUERY_ARGS
    )
    records = query.all()

    # Convert DB records to dataframe
    initial_df = db_to_df(records)

    # Select a number of random machines if specified
    if settings.NUMBER_OF_RANDOM_MACHINES:
        machines = random.sample(
            list(initial_df.hostname.unique()),
            k=settings.NUMBER_OF_RANDOM_MACHINES
        )
        initial_df = initial_df[initial_df.hostname.isin(machines)]

    # Normalize CPU values against specified thresholds if specified
    if settings.CPU_THRESHOLD_NORMALIZATION:
        normalized_cpu_df = normalized_cpu_threshold_df(
            initial_df,
            settings.CPU_THRESHOLD_NORMALIZATION
        )
    else:
        normalized_cpu_df = initial_df

    # Get tslearn compatible timeseries, with missing dates
    # filled automatically
    # Increase time series interval if specified
    timeseries = hosts_timeseries(
        normalized_cpu_df,
        interval_seconds=settings.INTERVAL_SECONDS
    )
    dataset_end = datetime.now()
    print('{} Dataset fetched'.format(dataset_end))
    dataset_duration = int((dataset_end - dataset_start).total_seconds())
    print('Dataset duration: {}s'.format(dataset_duration))
    print()

    clustering_start = datetime.now()
    print('{} Starting clustering'.format(clustering_start))
    tskm = run_clustering(timeseries, **settings.TSKM_ARGS)
    clustering_end = datetime.now()
    print('{} Finished clustering'.format(clustering_end))
    clustering_duration = int((clustering_end - clustering_start).total_seconds())
    print('Clustering duration: {}s'.format(clustering_duration))
    print()

    return tskm

def elbow(dic_settings, clusters_range):

    """
        Create elbow plot doing clusters following dic_settings parameters,
        but in a range of clusters given by clusters_range.
        clusters_range given as (2,10)
    """
    inertia_list = []
    K = range(clusters_range[0],clusters_range[1])
    for i in K:
        dic_settings['n_clusters'] = i
        tskm_result = only_clustering(dic_settings)
        # y_pred = tskm_result.labels_
        inertia_list.append(tskm_result.inertia_)
    
    #knee
    kn = KneeLocator(K, inertia_list, curve='convex', direction='decreasing')
    
    plt.figure(figsize=(16,8))
    plt.plot(K, inertia_list, 'bx-')
    
    #plot knee 
    plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')
    
    plt.xlabel('k')
    plt.ylabel('Distortion')
    plt.title('The Elbow Method showing the optimal k')

    direc = 'Results/Clustering results-Inertia_elbow/elbow.png'
    

    start = datetime.now()

    dataset_start = datetime.now()
    print('{} Fetching dataset'.format(dataset_start))
    # DB query
    query = get_cpu_query(
        **settings.DB_QUERY_ARGS
    )
    records = query.all()

    # Convert DB records to dataframe
    initial_df = db_to_df(records)

    # Select a number of random machines if specified
    if settings.NUMBER_OF_RANDOM_MACHINES:
        machines = random.sample(
            list(initial_df.hostname.unique()),
            k=settings.NUMBER_OF_RANDOM_MACHINES
        )
        initial_df = initial_df[initial_df.hostname.isin(machines)]

    # Normalize CPU values against specified thresholds if specified
    if settings.CPU_THRESHOLD_NORMALIZATION:
        normalized_cpu_df = normalized_cpu_threshold_df(
            initial_df,
            settings.CPU_THRESHOLD_NORMALIZATION
        )
    else:
        normalized_cpu_df = initial_df

    # Get tslearn compatible timeseries, with missing dates
    # filled automatically
    # Increase time series interval if specified
    timeseries = hosts_timeseries(
        normalized_cpu_df,
        interval_seconds=settings.INTERVAL_SECONDS
    )
    dataset_end = datetime.now()
    print('{} Dataset fetched'.format(dataset_end))
    dataset_duration = int((dataset_end - dataset_start).total_seconds())
    print('Dataset duration: {}s'.format(dataset_duration))
    print()

    clustering_start = datetime.now()
    print('{} Starting clustering'.format(clustering_start))
    tskm = run_clustering(timeseries, **settings.TSKM_ARGS)
    clustering_end = datetime.now()
    print('{} Finished clustering'.format(clustering_end))
    clustering_duration = int((clustering_end - clustering_start).total_seconds())
    print('Clustering duration: {}s'.format(clustering_duration))
    print()

    # Convert output directory to elbow path
    results_directory = os.path.abspath('tmp/elbow')

    # Create output directory if not existing
    if not os.path.isdir(results_directory):
        os.makedirs(results_directory)

    results_start = datetime.now()
    print('{} Writing results to {}'.format(results_start, results_directory))
    write_clustering_results(
        initial_df,
        normalized_cpu_df,
        tskm,
        results_directory
    )
    results_end = datetime.now()
    print('{} Results written'.format(results_end))
    results_duration = int((results_end - results_start).total_seconds())
    print('Results duration: {}s'.format(results_duration))
    print()

    end = datetime.now()
    duration = int((end - start).total_seconds())
    print('Finished')
    print('Total duration: {}s'.format(duration))

    # Write summary to output directory
    summary = {
        'start_date': start,
        'settings': {
            'DB_QUERY_ARGS': settings.DB_QUERY_ARGS,
            'NUMBER_OF_RANDOM_MACHINES': settings.NUMBER_OF_RANDOM_MACHINES,
            'CPU_THRESHOLD_NORMALIZATION': settings.CPU_THRESHOLD_NORMALIZATION,
            'INTERVAL_SECONDS': settings.INTERVAL_SECONDS,
            'TSKM_ARGS': settings.TSKM_ARGS,
            'RESULTS_DIRECTORY': settings.results_directory,
        },
        'durations': {
            'dataset': dataset_duration,
            'clustering': clustering_duration,
            'results': results_duration,
            'total': duration,
        }
    }
    with open(os.path.join(results_directory, 'summary.json'), 'w') as summary_file:
        json.dump(summary, summary_file, sort_keys=False, indent=4, default=_json_default)


# Run as program
if __name__ == '__main__':
    dataset_and_clustering()
